{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator KNeighborsClassifier from version 1.1.1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LinearSVC from version 1.1.1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/samsoup/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# load embeddings\n",
    "from data.embeddings import load_saved_embeddings\n",
    "\n",
    "train_embeddings = load_saved_embeddings(\n",
    "    dataset=\"esnli\",\n",
    "    model=\"deberta-large\",\n",
    "    seed=42,\n",
    "    split=\"train\",\n",
    "    pooler=\"mean_with_attention\",\n",
    "    layer=24\n",
    ")\n",
    "\n",
    "eval_embeddings = load_saved_embeddings(\n",
    "    dataset=\"esnli\",\n",
    "    model=\"deberta-large\",\n",
    "    seed=42,\n",
    "    split=\"eval\",\n",
    "    pooler=\"mean_with_attention\",\n",
    "    layer=24\n",
    ")\n",
    "\n",
    "test_embeddings = load_saved_embeddings(\n",
    "    dataset=\"esnli\",\n",
    "    model=\"deberta-large\",\n",
    "    seed=42,\n",
    "    split=\"test\",\n",
    "    pooler=\"mean_with_attention\",\n",
    "    layer=24\n",
    ")\n",
    "\n",
    "# load classifier \n",
    "from data.models import load_saved_wrapperbox_model\n",
    "\n",
    "\n",
    "knn_clf = load_saved_wrapperbox_model(\n",
    "    dataset=\"esnli\",\n",
    "    model=\"deberta-large\",\n",
    "    seed=42,\n",
    "    pooler=\"mean_with_attention\",\n",
    "    wrapperbox=\"KNN\"\n",
    ")\n",
    "\n",
    "svm_clf = load_saved_wrapperbox_model(\n",
    "    dataset=\"esnli\",\n",
    "    model=\"deberta-large\",\n",
    "    seed=42,\n",
    "    pooler=\"mean_with_attention\",\n",
    "    wrapperbox=\"SVM\",\n",
    ")\n",
    "\n",
    "dt_clf = load_saved_wrapperbox_model(\n",
    "    dataset=\"esnli\",\n",
    "    model=\"deberta-large\",\n",
    "    seed=42,\n",
    "    pooler=\"mean_with_attention\",\n",
    "    wrapperbox=\"DecisionTree\",\n",
    ")\n",
    "\n",
    "lmeans_clf = load_saved_wrapperbox_model(\n",
    "    dataset=\"esnli\",\n",
    "    model=\"deberta-large\",\n",
    "    seed=42,\n",
    "    pooler=\"mean_with_attention\",\n",
    "    wrapperbox=\"LMeans\",\n",
    ")\n",
    "\n",
    "# load labels \n",
    "from data.datasets import load_dataset_from_hf, load_labels_at_split\n",
    "import numpy as np\n",
    "dataset = load_dataset_from_hf(dataset=\"esnli\")\n",
    "train_labels = load_labels_at_split(dataset, \"train\")\n",
    "eval_labels = load_labels_at_split(dataset, \"eval\")\n",
    "train_eval_embeddings = np.vstack([train_embeddings, eval_embeddings])\n",
    "train_eval_labels = np.concatenate([train_labels, eval_labels])\n",
    "test_labels = load_labels_at_split(dataset, \"test\")\n",
    "\n",
    "from datasets import DatasetDict, concatenate_datasets\n",
    "train_eval_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"eval\"]])\n",
    "dataset_dict = DatasetDict(\n",
    "    {\"train\": train_eval_dataset, \"test\": dataset[\"test\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_0_f1': 0.9241727259237276,\n",
      " 'test_0_precision': 0.92376149510531,\n",
      " 'test_0_recall': 0.9245843230403801,\n",
      " 'test_1_f1': 0.8861586314152411,\n",
      " 'test_1_precision': 0.8872625350358144,\n",
      " 'test_1_recall': 0.8850574712643678,\n",
      " 'test_2_f1': 0.9498379379533879,\n",
      " 'test_2_precision': 0.9491054904380012,\n",
      " 'test_2_recall': 0.950571516836577,\n",
      " 'test_accuracy': 0.9201954397394136,\n",
      " 'test_f1': 0.9200564317641189,\n",
      " 'test_macro_f1': 0.9200564317641189,\n",
      " 'test_macro_precision': 0.9200431735263752,\n",
      " 'test_macro_recall': 0.920071103713775,\n",
      " 'test_micro_f1': 0.9201954397394136,\n",
      " 'test_micro_precision': 0.9201954397394136,\n",
      " 'test_micro_recall': 0.9201954397394136,\n",
      " 'test_precision': 0.9200431735263752,\n",
      " 'test_recall': 0.920071103713775,\n",
      " 'test_weighted_f1': 0.9201734304348423,\n",
      " 'test_weighted_precision': 0.9201528184388009,\n",
      " 'test_weighted_recall': 0.9201954397394136}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      3368\n",
      "           1       0.89      0.89      0.89      3219\n",
      "           2       0.95      0.95      0.95      3237\n",
      "\n",
      "    accuracy                           0.92      9824\n",
      "   macro avg       0.92      0.92      0.92      9824\n",
      "weighted avg       0.92      0.92      0.92      9824\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from utils.inference import compute_metrics\n",
    "\n",
    "l2 = 500\n",
    "logit_clf = LogisticRegression(penalty=\"l2\", C= 1 / l2)\n",
    "\n",
    "logit_clf.fit(train_eval_embeddings, train_eval_labels)\n",
    "\n",
    "predictions = logit_clf.predict(test_embeddings)\n",
    "\n",
    "# Print some metrics\n",
    "testset_perfm = compute_metrics(\n",
    "    y_true=test_labels, y_pred=predictions, is_multiclass=True, prefix=\"test\"\n",
    ")\n",
    "pprint(testset_perfm)\n",
    "print(classification_report(y_true=test_labels, y_pred=predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrapperbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
