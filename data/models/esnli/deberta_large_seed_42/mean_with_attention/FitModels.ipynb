{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import classification_report\n",
    "from utils.inference import compute_metrics\n",
    "\n",
    "\n",
    "DATASET_NAME = \"esnli\"\n",
    "LABEL_SPACE = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "MODEL_NAME = \"deberta_large\"\n",
    "SEED = 42\n",
    "POOLER = \"mean_with_attention\"\n",
    "LAYER = 24\n",
    "\n",
    "def evaluate(y_true, y_pred, is_multiclass: bool, prefix: str='test'):\n",
    "    # Print some metrics\n",
    "    testset_perfm = compute_metrics(\n",
    "        y_true=y_true, y_pred=y_pred, is_multiclass=is_multiclass, prefix=prefix\n",
    "    )\n",
    "    pprint(testset_perfm)\n",
    "    print(classification_report(y_true=y_true, y_pred=y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/samsoup/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Load Embeddings\n",
    "from utils.io import (\n",
    "    load_dataset_from_hf,\n",
    "    load_labels_at_split,\n",
    "    load_embeddings,\n",
    ")\n",
    "import numpy as np\n",
    "train_embeddings = load_embeddings(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    split=\"train\",\n",
    "    pooler=POOLER,\n",
    "    layer=LAYER\n",
    ")\n",
    "\n",
    "eval_embeddings = load_embeddings(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    split=\"eval\",\n",
    "    pooler=POOLER,\n",
    "    layer=LAYER\n",
    ")\n",
    "\n",
    "test_embeddings = load_embeddings(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    split=\"test\",\n",
    "    pooler=POOLER,\n",
    "    layer=LAYER\n",
    ")\n",
    "\n",
    "train_eval_embeddings = np.vstack([train_embeddings, eval_embeddings])\n",
    "\n",
    "## Load Datasets and Labels\n",
    "dataset = load_dataset_from_hf(dataset=DATASET_NAME)\n",
    "train_labels = load_labels_at_split(dataset, \"train\")\n",
    "eval_labels = load_labels_at_split(dataset, \"eval\")\n",
    "train_eval_labels = np.concatenate([train_labels, eval_labels])\n",
    "test_labels = load_labels_at_split(dataset, \"test\")\n",
    "\n",
    "from datasets import DatasetDict, concatenate_datasets\n",
    "train_eval_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"eval\"]])\n",
    "dataset_dict = DatasetDict(\n",
    "    {\"train\": train_eval_dataset, \"test\": dataset[\"test\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_0_f1': 0.9247216035634744,\n",
      " 'test_0_precision': 0.9248589248589248,\n",
      " 'test_0_recall': 0.9245843230403801,\n",
      " 'test_1_f1': 0.8873065015479876,\n",
      " 'test_1_precision': 0.8842949706880593,\n",
      " 'test_1_recall': 0.8903386144765455,\n",
      " 'test_2_f1': 0.9499457616612428,\n",
      " 'test_2_precision': 0.9530472636815921,\n",
      " 'test_2_recall': 0.9468643805993203,\n",
      " 'test_accuracy': 0.9207043973941368,\n",
      " 'test_f1': 0.9206579555909015,\n",
      " 'test_macro_f1': 0.9206579555909015,\n",
      " 'test_macro_precision': 0.9207337197428588,\n",
      " 'test_macro_recall': 0.9205957727054153,\n",
      " 'test_micro_f1': 0.9207043973941368,\n",
      " 'test_micro_precision': 0.9207043973941368,\n",
      " 'test_micro_recall': 0.9207043973941368,\n",
      " 'test_precision': 0.9207337197428588,\n",
      " 'test_recall': 0.9205957727054153,\n",
      " 'test_weighted_f1': 0.920773251199328,\n",
      " 'test_weighted_precision': 0.9208554928854882,\n",
      " 'test_weighted_recall': 0.9207043973941368}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      3368\n",
      "           1       0.88      0.89      0.89      3219\n",
      "           2       0.95      0.95      0.95      3237\n",
      "\n",
      "    accuracy                           0.92      9824\n",
      "   macro avg       0.92      0.92      0.92      9824\n",
      "weighted avg       0.92      0.92      0.92      9824\n",
      "\n",
      "Model saved to LogisticRegression.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "l2 = 1\n",
    "logit_clf = LogisticRegression(penalty=\"l2\", C= 1 / l2, max_iter=1000)\n",
    "logit_clf.fit(train_eval_embeddings, train_eval_labels)\n",
    "predictions = logit_clf.predict(test_embeddings)\n",
    "\n",
    "\n",
    "evaluate(\n",
    "    y_pred=predictions, \n",
    "    y_true=test_labels, \n",
    "    is_multiclass=np.unique(test_labels).size > 2\n",
    ")\n",
    "\n",
    "# Path to save the model\n",
    "model_path = 'LogisticRegression.pkl'\n",
    "\n",
    "# Save the trained model to a .pkl file\n",
    "with open(model_path, 'wb') as file:\n",
    "    pickle.dump(logit_clf, file)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'thresh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlogit_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthresh\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'thresh'"
     ]
    }
   ],
   "source": [
    "logit_clf.thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_0_f1': 0.34888922021768304,\n",
      " 'test_0_precision': 0.3504043126684636,\n",
      " 'test_0_recall': 0.3473871733966746,\n",
      " 'test_1_f1': 0.3348751156336725,\n",
      " 'test_1_precision': 0.33241505968778695,\n",
      " 'test_1_recall': 0.33737185461323393,\n",
      " 'test_2_f1': 0.32285050348567,\n",
      " 'test_2_precision': 0.3238036047234307,\n",
      " 'test_2_recall': 0.3219029966017918,\n",
      " 'test_accuracy': 0.3357084690553746,\n",
      " 'test_f1': 0.3355382797790085,\n",
      " 'test_macro_f1': 0.3355382797790085,\n",
      " 'test_macro_precision': 0.33554099235989376,\n",
      " 'test_macro_recall': 0.3355540082039001,\n",
      " 'test_micro_f1': 0.3357084690553746,\n",
      " 'test_micro_precision': 0.3357084690553746,\n",
      " 'test_micro_recall': 0.3357084690553746,\n",
      " 'test_precision': 0.33554099235989376,\n",
      " 'test_recall': 0.3355540082039001,\n",
      " 'test_weighted_f1': 0.3357175255192449,\n",
      " 'test_weighted_precision': 0.3357449176193115,\n",
      " 'test_weighted_recall': 0.3357084690553746}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.35      0.35      3368\n",
      "           1       0.33      0.34      0.33      3219\n",
      "           2       0.32      0.32      0.32      3237\n",
      "\n",
      "    accuracy                           0.34      9824\n",
      "   macro avg       0.34      0.34      0.34      9824\n",
      "weighted avg       0.34      0.34      0.34      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from classifiers.RandomClassifier import RandomClassifier\n",
    "\n",
    "\n",
    "random_clf = RandomClassifier()\n",
    "random_clf.fit(train_eval_embeddings, train_eval_labels)\n",
    "predictions = random_clf.predict(test_embeddings)\n",
    "\n",
    "evaluate(\n",
    "    y_pred=predictions, \n",
    "    y_true=test_labels, \n",
    "    is_multiclass=np.unique(test_labels).size > 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.165868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 559203, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score -1.096779\n",
      "[LightGBM] [Info] Start training from score -1.100782\n",
      "[LightGBM] [Info] Start training from score -1.098280\n",
      "{'test_0_f1': 0.9010137149672034,\n",
      " 'test_0_precision': 0.9047904191616767,\n",
      " 'test_0_recall': 0.8972684085510689,\n",
      " 'test_1_f1': 0.8536133415688697,\n",
      " 'test_1_precision': 0.8486337120049124,\n",
      " 'test_1_recall': 0.8586517552034794,\n",
      " 'test_2_f1': 0.9260519801980198,\n",
      " 'test_2_precision': 0.9274868298729471,\n",
      " 'test_2_recall': 0.92462156317578,\n",
      " 'test_accuracy': 0.8936278501628665,\n",
      " 'test_f1': 0.8935596789113642,\n",
      " 'test_macro_f1': 0.8935596789113642,\n",
      " 'test_macro_precision': 0.8936369870131786,\n",
      " 'test_macro_recall': 0.8935139089767761,\n",
      " 'test_micro_f1': 0.8936278501628665,\n",
      " 'test_micro_precision': 0.8936278501628665,\n",
      " 'test_micro_recall': 0.8936278501628665,\n",
      " 'test_precision': 0.8936369870131786,\n",
      " 'test_recall': 0.8935139089767761,\n",
      " 'test_weighted_f1': 0.8937322677545525,\n",
      " 'test_weighted_precision': 0.8938681717201822,\n",
      " 'test_weighted_recall': 0.8936278501628665}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90      3368\n",
      "           1       0.85      0.86      0.85      3219\n",
      "           2       0.93      0.92      0.93      3237\n",
      "\n",
      "    accuracy                           0.89      9824\n",
      "   macro avg       0.89      0.89      0.89      9824\n",
      "weighted avg       0.89      0.89      0.89      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMModel\n",
    "\n",
    "num_labels = np.unique(test_labels).size\n",
    "objective='binary' if num_labels == 2 else 'multiclass'\n",
    "clf = LGBMModel(\n",
    "    objective=objective,\n",
    "    num_classes=1 if objective==\"binary\" else num_labels,\n",
    "    learning_rate=1.0,      # Set to 1.0 for a single tree\n",
    "    n_estimators=1,         # Build only one tree\n",
    "    min_child_samples=20,   # minimum samples in leaf \n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(train_eval_embeddings, train_eval_labels)\n",
    "\n",
    "prediction_probas = clf.predict(test_embeddings)\n",
    "if objective == \"binary\":\n",
    "    predictions = (prediction_probas >= 0.5).astype(int) # threshold using 0.5\n",
    "else:\n",
    "    predictions = np.argmax(prediction_probas, axis=1)\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import classification_report\n",
    "from utils.inference import compute_metrics\n",
    "\n",
    "# Print some metrics\n",
    "testset_perfm = compute_metrics(\n",
    "    y_true=test_labels, y_pred=predictions, is_multiclass=objective==\"multiclass\", prefix=\"test\"\n",
    ")\n",
    "pprint(testset_perfm)\n",
    "print(classification_report(y_true=test_labels, y_pred=predictions))\n",
    "\n",
    "import pickle\n",
    "# Save model to file\n",
    "model_filename = 'LGBM.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(clf, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_0_f1': 0.9159077109896782,\n",
      " 'test_0_precision': 0.9369565217391305,\n",
      " 'test_0_recall': 0.8957838479809976,\n",
      " 'test_1_f1': 0.8747252747252747,\n",
      " 'test_1_precision': 0.8277870216306157,\n",
      " 'test_1_recall': 0.9273066169617894,\n",
      " 'test_2_f1': 0.9395348837209302,\n",
      " 'test_2_precision': 0.9769846564376251,\n",
      " 'test_2_recall': 0.9048501699104109,\n",
      " 'test_accuracy': 0.9091001628664495,\n",
      " 'test_f1': 0.9100559564786277,\n",
      " 'test_macro_f1': 0.9100559564786277,\n",
      " 'test_macro_precision': 0.9139093999357905,\n",
      " 'test_macro_recall': 0.909313544951066,\n",
      " 'test_micro_f1': 0.9091001628664495,\n",
      " 'test_micro_precision': 0.9091001628664495,\n",
      " 'test_micro_recall': 0.9091001628664495,\n",
      " 'test_precision': 0.9139093999357905,\n",
      " 'test_recall': 0.909313544951066,\n",
      " 'test_weighted_f1': 0.910198722369559,\n",
      " 'test_weighted_precision': 0.9143745236904454,\n",
      " 'test_weighted_recall': 0.9091001628664495}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92      3368\n",
      "           1       0.83      0.93      0.87      3219\n",
      "           2       0.98      0.90      0.94      3237\n",
      "\n",
      "    accuracy                           0.91      9824\n",
      "   macro avg       0.91      0.91      0.91      9824\n",
      "weighted avg       0.91      0.91      0.91      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from classifiers.KMeansClassifier import KMeansClassifier\n",
    "\n",
    "\n",
    "clf = KMeansClassifier(algorithm='elkan', init='k-means++', n_clusters=3,random_state=42)\n",
    "clf.fit(train_eval_embeddings, train_eval_labels)\n",
    "predictions = clf.predict(test_embeddings)\n",
    "\n",
    "evaluate(\n",
    "    y_pred=predictions, \n",
    "    y_true=test_labels, \n",
    "    is_multiclass=np.unique(test_labels).size > 2\n",
    ")\n",
    "\n",
    "import pickle\n",
    "# Save model to file\n",
    "model_filename = 'LMeans.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrapperbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
