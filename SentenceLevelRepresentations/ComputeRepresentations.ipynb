{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained model tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "MAX_LEN = 512\n",
    "def extract_features(text):\n",
    "    input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True, max_length=MAX_LEN, truncation=True)])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        cls_hidden_state = outputs.last_hidden_state[:, 0, :]  # Extract the [CLS] token's hidden state\n",
    "    return cls_hidden_state.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11678/11678 [27:22<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train data to /home/samsoup/Work/WrapperBox/data/embeddings/essays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1298/1298 [03:32<00:00,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test data to /home/samsoup/Work/WrapperBox/data/embeddings/essays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the essays dataset \n",
    "from tqdm import tqdm\n",
    "from utils.constants.directory import WORK_DIR, EMBEDDINGS_DIR\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "NAME = \"essays\"\n",
    "INPUT_COL = \"essay\"\n",
    "OUTPUT_DIR = os.path.join(EMBEDDINGS_DIR, NAME)\n",
    "\n",
    "dir = os.path.join(WORK_DIR, f\"data/datasets/{NAME}\")\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(dir, \"train.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(dir, \"test.csv\"))\n",
    "\n",
    "# Extract features\n",
    "splits = [(\"train\", train_data), (\"test\", test_data)]\n",
    "\n",
    "for split, data in splits:\n",
    "    features = []\n",
    "    for sentence in tqdm(data[INPUT_COL]):\n",
    "        f = extract_features(sentence)\n",
    "        features.append(f)\n",
    "\n",
    "    # Save the extracted features\n",
    "    np.save(os.path.join(OUTPUT_DIR, f\"{split}.npy\"), np.array(features))\n",
    "    print(f\"Saved {split} data to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18000/18000 [10:56<00:00, 27.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train data to /home/samsoup/Work/WrapperBox/data/embeddings/tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:37<00:00, 26.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test data to /home/samsoup/Work/WrapperBox/data/embeddings/tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the tweets dataset \n",
    "from tqdm import tqdm\n",
    "from utils.constants.directory import WORK_DIR, EMBEDDINGS_DIR\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "NAME = \"tweets\"\n",
    "INPUT_COL = \"text\"\n",
    "OUTPUT_DIR = os.path.join(EMBEDDINGS_DIR, NAME)\n",
    "\n",
    "dir = os.path.join(WORK_DIR, f\"data/datasets/{NAME}\")\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(dir, \"train.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(dir, \"test.csv\"))\n",
    "\n",
    "# Extract features for the Essay dataset\n",
    "\n",
    "splits = [(\"train\", train_data), (\"test\", test_data)]\n",
    "\n",
    "for split, data in splits:\n",
    "    features = []\n",
    "    for sentence in tqdm(data[INPUT_COL]):\n",
    "        f = extract_features(sentence)\n",
    "        features.append(f)\n",
    "\n",
    "    # Save the extracted features\n",
    "    np.save(os.path.join(OUTPUT_DIR, f\"{split}.npy\"), np.array(features))\n",
    "    print(f\"Saved {split} data to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9632 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9632/9632 [08:41<00:00, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train data to /home/samsoup/Work/WrapperBox/data/embeddings/hatespeech\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [00:58<00:00, 18.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test data to /home/samsoup/Work/WrapperBox/data/embeddings/hatespeech\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the hapespeech dataset \n",
    "from tqdm import tqdm\n",
    "from utils.constants.directory import WORK_DIR, EMBEDDINGS_DIR\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "NAME = \"hatespeech\"\n",
    "INPUT_COL = \"text\"\n",
    "OUTPUT_DIR = os.path.join(EMBEDDINGS_DIR, NAME)\n",
    "\n",
    "dir = os.path.join(WORK_DIR, f\"data/datasets/{NAME}\")\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(dir, \"train.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(dir, \"test.csv\"))\n",
    "\n",
    "# Extract features for the Essay dataset\n",
    "\n",
    "splits = [(\"train\", train_data), (\"test\", test_data)]\n",
    "\n",
    "for split, data in splits:\n",
    "    features = []\n",
    "    for sentence in tqdm(data[INPUT_COL]):\n",
    "        f = extract_features(sentence)\n",
    "        features.append(f)\n",
    "\n",
    "    # Save the extracted features\n",
    "    np.save(os.path.join(OUTPUT_DIR, f\"{split}.npy\"), np.array(features))\n",
    "    print(f\"Saved {split} data to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9025 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9025/9025 [05:39<00:00, 26.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train data to /home/samsoup/Work/WrapperBox/data/embeddings/emotion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1003/1003 [00:41<00:00, 24.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test data to /home/samsoup/Work/WrapperBox/data/embeddings/emotion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the emotion dataset \n",
    "from tqdm import tqdm\n",
    "from utils.constants.directory import WORK_DIR, EMBEDDINGS_DIR\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "NAME = \"emotion\"\n",
    "INPUT_COL = \"text\"\n",
    "OUTPUT_DIR = os.path.join(EMBEDDINGS_DIR, NAME)\n",
    "\n",
    "dir = os.path.join(WORK_DIR, f\"data/datasets/{NAME}\")\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(dir, \"train.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(dir, \"test.csv\"))\n",
    "\n",
    "# Extract features for the Essay dataset\n",
    "\n",
    "splits = [(\"train\", train_data), (\"test\", test_data)]\n",
    "\n",
    "for split, data in splits:\n",
    "    features = []\n",
    "    for sentence in tqdm(data[INPUT_COL]):\n",
    "        f = extract_features(sentence)\n",
    "        features.append(f)\n",
    "\n",
    "    # Save the extracted features\n",
    "    np.save(os.path.join(OUTPUT_DIR, f\"{split}.npy\"), np.array(features))\n",
    "    print(f\"Saved {split} data to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6920/6920 [04:03<00:00, 28.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train data to /home/samsoup/Work/WrapperBox/data/embeddings/sst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 872/872 [00:31<00:00, 27.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test data to /home/samsoup/Work/WrapperBox/data/embeddings/sst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the emotion dataset \n",
    "from tqdm import tqdm\n",
    "from utils.constants.directory import WORK_DIR, EMBEDDINGS_DIR\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "NAME = \"sst\"\n",
    "INPUT_COL = \"document\"\n",
    "OUTPUT_DIR = os.path.join(EMBEDDINGS_DIR, NAME)\n",
    "\n",
    "dir = os.path.join(WORK_DIR, f\"data/datasets/{NAME}\")\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(dir, \"train.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(dir, \"test.csv\"))\n",
    "\n",
    "# Extract features for the Essay dataset\n",
    "\n",
    "splits = [(\"train\", train_data), (\"test\", test_data)]\n",
    "\n",
    "for split, data in splits:\n",
    "    features = []\n",
    "    for sentence in tqdm(data[INPUT_COL]):\n",
    "        f = extract_features(sentence)\n",
    "        features.append(f)\n",
    "\n",
    "    # Save the extracted features\n",
    "    np.save(os.path.join(OUTPUT_DIR, f\"{split}.npy\"), np.array(features))\n",
    "    print(f\"Saved {split} data to {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrapperbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
