{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import classification_report\n",
    "from ExampleBasedExplanations.factory import ExampleBasedExplanationFactory\n",
    "from utils.inference import compute_metrics\n",
    "from utils.io import mkdir_if_not_exists\n",
    "\n",
    "## NOTE: CHANGE here to for future runs\n",
    "EMBEDDING_PATH = \"/home/samsoup/Work/WrapperBox/SentenceLevelRepresentations/AnubrataQA/embeddings/T5\"\n",
    "DATASET_PATH = \"anubrata/south-german-credit\"\n",
    "DATASET_NAME = \"south-german-credit\"\n",
    "RANDOM_STATE = 42 # for deterministic results\n",
    "M = 5\n",
    "\n",
    "def evaluate(y_true, y_pred, is_multiclass: bool, prefix: str='test'):\n",
    "    # Print some metrics\n",
    "    testset_perfm = compute_metrics(\n",
    "        y_true=y_true, y_pred=y_pred, is_multiclass=is_multiclass, prefix=prefix\n",
    "    )\n",
    "    pprint(testset_perfm)\n",
    "    print(classification_report(y_true=y_true, y_pred=y_pred))\n",
    "\n",
    "def get_and_save_example_based_explanations(\n",
    "    wrapper_name: str, M: int, clf: BaseEstimator, \n",
    "    train_embeddings: np.ndarray,\n",
    "    test_embeddings: np.ndarray,\n",
    "    output_dir: str = None,\n",
    "    dataset=None,\n",
    "    dataset_name=None,\n",
    "    model_name=None,\n",
    "    predictions=None,\n",
    "):\n",
    "    factory = ExampleBasedExplanationFactory()\n",
    "    handler_class = factory.get_handler(\n",
    "        f\"{wrapper_name}{factory.interface_name}\"\n",
    "    )\n",
    "    handler = handler_class()\n",
    "    expl_indices = handler.get_explanation_indices(\n",
    "        M=M,\n",
    "        clf=clf,\n",
    "        train_embeddings=train_embeddings,\n",
    "        test_embeddings=test_embeddings,\n",
    "    )\n",
    "    \n",
    "    if output_dir is not None:\n",
    "        mkdir_if_not_exists(output_dir)\n",
    "\n",
    "        handler.persist_to_disk(\n",
    "            dataset=dataset,\n",
    "            dataset_name=dataset_name,\n",
    "            model_name=model_name,\n",
    "            wrapper_name=wrapper_name,\n",
    "            predictions=predictions,\n",
    "            explanation_indices=expl_indices,\n",
    "            output_dir=output_dir,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from data.datasets.load_dataset import load_labels_at_split\n",
    "from utils.constants.directory import CACHE_DIR\n",
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load Embeddings\n",
    "train_embeddings = np.load(\n",
    "    f\"{EMBEDDING_PATH}/train_representations.npy\"\n",
    ")\n",
    "eval_embeddings = np.load(\n",
    "    f\"{EMBEDDING_PATH}/val_representations.npy\"\n",
    ")\n",
    "test_embeddings = np.load(\n",
    "    f\"{EMBEDDING_PATH}/test_representations.npy\"\n",
    ")\n",
    "train_eval_embeddings = np.vstack([train_embeddings, eval_embeddings])\n",
    "\n",
    "# Load datasets \n",
    "# load from parquet if full path, otherwise huggingface\n",
    "# NOTE: assumes at least two column: 'text' and 'label'\n",
    "if os.path.isabs(DATASET_PATH):\n",
    "    train_dataset = Dataset.from_pandas(\n",
    "        pd.read_parquet(f\"{DATASET_PATH}/train.parquet\")\n",
    "    )\n",
    "    valid_dataset = Dataset.from_pandas(\n",
    "        pd.read_parquet(f\"{DATASET_PATH}/val.parquet\")\n",
    "    )\n",
    "    test_dataset = Dataset.from_pandas(\n",
    "        pd.read_parquet(f\"{DATASET_PATH}/test.parquet\")\n",
    "    )\n",
    "else:\n",
    "    dataset_dict = load_dataset(\n",
    "        f\"{DATASET_PATH}\", cache_dir=CACHE_DIR, \n",
    "        token=\"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\"\n",
    "    )\n",
    "\n",
    "# Create the dataset with combined train and eval, for later use\n",
    "train_labels = load_labels_at_split(dataset_dict, \"train\")\n",
    "eval_labels = load_labels_at_split(dataset_dict, \"val\")\n",
    "test_labels = load_labels_at_split(dataset_dict, \"test\")\n",
    "# Check labels and convert to numeric if not already \n",
    "if isinstance(train_labels[0], str):\n",
    "    le = LabelEncoder()\n",
    "    train_labels = le.fit_transform(train_labels)\n",
    "    dataset_dict['train'] = dataset_dict['train'].map(\n",
    "        lambda example: \n",
    "            {f\"label\": le.fit_transform([example['label']])[0]}, \n",
    "        batched=False)\n",
    "\n",
    "    eval_labels = le.fit_transform(eval_labels)\n",
    "    dataset_dict['val'] = dataset_dict['val'].map(\n",
    "        lambda example: \n",
    "            {f\"label\": le.fit_transform([example['label']])[0]}, \n",
    "        batched=False)\n",
    "\n",
    "    test_labels = le.fit_transform(test_labels)\n",
    "    dataset_dict['test'] = dataset_dict['test'].map(\n",
    "        lambda example: \n",
    "            {f\"label\": le.fit_transform([example['label']])[0]}, \n",
    "        batched=False)\n",
    "\n",
    "\n",
    "train_eval_labels = np.concatenate([train_labels, eval_labels])\n",
    "new_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": concatenate_datasets(\n",
    "            [dataset_dict[\"train\"], dataset_dict[\"val\"]]\n",
    "        ),\n",
    "        \"test\": dataset_dict[\"test\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.675,\n",
      " 'test_f1': 0.758364312267658,\n",
      " 'test_precision': 0.7786259541984732,\n",
      " 'test_recall': 0.7391304347826086}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.53      0.50        62\n",
      "           1       0.78      0.74      0.76       138\n",
      "\n",
      "    accuracy                           0.68       200\n",
      "   macro avg       0.63      0.64      0.63       200\n",
      "weighted avg       0.69      0.68      0.68       200\n",
      "\n",
      "AnubrataQA/results/south-german-credit_T5_KNN_explanations.json saved\n",
      "AnubrataQA/results/south-german-credit_T5_KNN_explanations.csv saved\n"
     ]
    }
   ],
   "source": [
    "## KNN\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from utils.inference import cross_validation_with_grid_search\n",
    "\n",
    "# hyper-params here\n",
    "wrapper_name = \"KNN\"\n",
    "n_neighbors = 5\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "# Fit on train + eval, can be changed if want to evaluate on val set too\n",
    "knn_clf.fit(train_eval_embeddings, train_eval_labels)\n",
    "predictions = knn_clf.predict(test_embeddings)\n",
    "\n",
    "evaluate(\n",
    "    y_pred=predictions, \n",
    "    y_true=test_labels, \n",
    "    is_multiclass=np.unique(test_labels).size > 2\n",
    ")\n",
    "\n",
    "get_and_save_example_based_explanations(\n",
    "    wrapper_name=wrapper_name, \n",
    "    M=M, \n",
    "    clf=knn_clf, \n",
    "    train_embeddings=train_eval_embeddings,\n",
    "    test_embeddings=test_embeddings,\n",
    "    output_dir=\"AnubrataQA/results\",\n",
    "    dataset=new_dataset,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    model_name=\"T5\",\n",
    "    predictions=predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.69,\n",
      " 'test_f1': 0.8165680473372781,\n",
      " 'test_precision': 0.69,\n",
      " 'test_recall': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        62\n",
      "           1       0.69      1.00      0.82       138\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.34      0.50      0.41       200\n",
      "weighted avg       0.48      0.69      0.56       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnubrataQA/results/south-german-credit_T5_SVM_explanations.json saved\n",
      "AnubrataQA/results/south-german-credit_T5_SVM_explanations.csv saved\n"
     ]
    }
   ],
   "source": [
    "## SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# hyper-params here\n",
    "wrapper_name = \"SVM\"\n",
    "\n",
    "svm_clf = LinearSVC(random_state=RANDOM_STATE)\n",
    "# Fit on train + eval, can be changed if want to evaluate on val set too\n",
    "svm_clf.fit(train_eval_embeddings, train_eval_labels)\n",
    "predictions = svm_clf.predict(test_embeddings)\n",
    "\n",
    "evaluate(\n",
    "    y_pred=predictions, \n",
    "    y_true=test_labels, \n",
    "    is_multiclass=np.unique(test_labels).size > 2\n",
    ")\n",
    "\n",
    "get_and_save_example_based_explanations(\n",
    "    wrapper_name=wrapper_name, \n",
    "    M=M, \n",
    "    clf=svm_clf, \n",
    "    train_embeddings=train_eval_embeddings,\n",
    "    test_embeddings=test_embeddings,\n",
    "    output_dir=\"AnubrataQA/results\",\n",
    "    dataset=new_dataset,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    model_name=\"T5\",\n",
    "    predictions=predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.7,\n",
      " 'test_f1': 0.7794117647058824,\n",
      " 'test_precision': 0.7910447761194029,\n",
      " 'test_recall': 0.7681159420289855}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.55      0.53        62\n",
      "           1       0.79      0.77      0.78       138\n",
      "\n",
      "    accuracy                           0.70       200\n",
      "   macro avg       0.65      0.66      0.66       200\n",
      "weighted avg       0.71      0.70      0.70       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Expl Indices for DT Manually: 100%|██████████| 200/200 [00:00<00:00, 4599.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnubrataQA/results/south-german-credit_T5_DecisionTree_explanations.json saved\n",
      "AnubrataQA/results/south-german-credit_T5_DecisionTree_explanations.csv saved\n"
     ]
    }
   ],
   "source": [
    "## DT\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# hyper-params here\n",
    "wrapper_name = \"DecisionTree\"\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "# Fit on train + eval, can be changed if want to evaluate on val set too\n",
    "dt_clf.fit(train_eval_embeddings, train_eval_labels)\n",
    "predictions = dt_clf.predict(test_embeddings)\n",
    "\n",
    "evaluate(\n",
    "    y_pred=predictions,\n",
    "    y_true=test_labels,\n",
    "    is_multiclass=np.unique(test_labels).size > 2\n",
    ")\n",
    "\n",
    "get_and_save_example_based_explanations(\n",
    "    wrapper_name=wrapper_name, \n",
    "    M=M, \n",
    "    clf=dt_clf, \n",
    "    train_embeddings=train_eval_embeddings,\n",
    "    test_embeddings=test_embeddings,\n",
    "    output_dir=\"AnubrataQA/results\",\n",
    "    dataset=new_dataset,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    model_name=\"T5\",\n",
    "    predictions=predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.69,\n",
      " 'test_f1': 0.8165680473372781,\n",
      " 'test_precision': 0.69,\n",
      " 'test_recall': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        62\n",
      "           1       0.69      1.00      0.82       138\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.34      0.50      0.41       200\n",
      "weighted avg       0.48      0.69      0.56       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnubrataQA/results/south-german-credit_T5_KMedoids_explanations.json saved\n",
      "AnubrataQA/results/south-german-credit_T5_KMedoids_explanations.csv saved\n"
     ]
    }
   ],
   "source": [
    "from classifiers.KMedoidsClassifier import KMedoidsClassifier\n",
    "\n",
    "# Hyper params\n",
    "wrapper_name = \"KMedoids\"\n",
    "n_clusters = 5\n",
    "\n",
    "kmed_clf = KMedoidsClassifier(n_clusters=n_clusters)\n",
    "kmed_clf.fit(train_eval_embeddings, train_eval_labels)\n",
    "predictions = kmed_clf.predict(test_embeddings)\n",
    "\n",
    "evaluate(\n",
    "    y_pred=predictions,\n",
    "    y_true=test_labels,\n",
    "    is_multiclass=np.unique(test_labels).size > 2\n",
    ")\n",
    "\n",
    "get_and_save_example_based_explanations(\n",
    "    wrapper_name=wrapper_name, \n",
    "    M=M, \n",
    "    clf=kmed_clf, \n",
    "    train_embeddings=train_eval_embeddings,\n",
    "    test_embeddings=test_embeddings,\n",
    "    output_dir=\"AnubrataQA/results\",\n",
    "    dataset=new_dataset,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    model_name=\"T5\",\n",
    "    predictions=predictions,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrapperbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
