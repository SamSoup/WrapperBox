project_name: "mpnet_base_ft_toxigen" # for wandb

model:
  name: "all-MiniLM-L6-v2"
  encoder_batch_size: 16
  head_params:
    output_dim: 2
    dropout_rate: 0.1

dataset:
  name: "Samsoup/toxigen"
  input_column: "text"
  label_column: "label"
  num_samples: 32
  max_seq_length: 512

metrics: 
  scores: ["accuracy", "precision", "recall", "f1"]
  average: "binary"

prompt:
  use_prompt: true
  template: "Instruct: {instruction}\nText: {input}" # Input will come from dataset
  params:
    instruction: "Given a text input, determine whether the content is toxic or benign for binary toxicity detection."

# Full list at: https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments
# I have ignored some of the arguments I think I will never use 
training_args:
  seed: 42
  dataloader_drop_last: False # do not drop last incomplete batch
  # Directory
  output_dir: './results'
  overwrite_output_dir: True
  # the Dos are not directly used, but are useful when scripting
  do_train: true
  do_eval: true
  do_predict: true
  # Optimization parameters
  optim: "adamw_torch"
  learning_rate: 0.00001
  weight_decay: 0.0  # for AdamW
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 0.00000001
  max_grad_norm: 1.0 # for gradient clipping
  warmup_ratio: 0.01 # total training steps for linear warmup
  warmup_steps: 0 # overrides warm_up ratio
  # Scheduling
  num_train_epochs: 2.0 
  max_steps: -1
  lr_scheduler_type: "linear"
  lr_scheduler_kwargs: null
  # Evaluation
  evaluation_strategy: "steps" # must match save_strategy
  eval_steps: 1000
  eval_delay: 0 # # of epochs/steps to delay first evaluation
  eval_accumulation_steps: null
  remove_unused_columns: False # NOTE: we are using a custom dataset, so this should be turned off
  load_best_model_at_end: True # Load best model at the end
  metric_for_best_model: "eval_f1"
  greater_is_better: True
  # Memory-impactful parameters
  use_cpu: False
  gradient_checkpointing: False
  # auto_find_batch_size: True # auto find batch sizes
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 1 # Accumulation steps before back-prop
  ## FP-16
  fp16: False
  fp16_opt_level: 'O1'
  fp16_backend: "auto"
  half_precision_backend: "auto"
  fp16_full_eval: False
  ## BF-16
  bf16: False # Requires Ampere or higher if true
  bf16_full_eval: False
  # Logging
  report_to: "wandb"
  run_name: "mnpet_test"
  log_level: "info"
  log_on_each_node: true
  logging_dir: null # tensor board logging dir
  logging_strategy: "steps"
  logging_first_step: False
  logging_steps: 100
  logging_nan_inf_filter: False
  # Saving
  save_strategy: "steps"
  save_steps: 1000
  save_total_limit: 1
  save_safetensors: True
  save_on_each_node: False
  save_only_model: True # Do not save optimizer, schduler, rng state
  load_best_model_at_end: True
  ## Hub, other arguments ignored since not used
  push_to_hub: False 
  ## Multinode
  local_rank: -1
  ddp_backend: null
  dataloader_num_workers: 0
  ## FSDP
  fsdp: False
  fsdp_config: null
  ## Deepspeed
  deepspeed: False
  accelerator_config: null

early_stopping:
  enabled: true
  patience: 2
  metric: "eval_f1"
  mode: "max"

logging:
  wandb:
    enabled: true
    project_name: "setfit-finetuning-mpnet"
    entity: "samsoup"

loss_function:
  type: "CrossEntropyLoss"
  params: 
    # weight: [0.75, 0.25]  # Example weights for two classes
    weight: [0.6606095 , 2.05657042] # Based on sklearn.utils.class_weight