{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Approaches\n",
    "\n",
    "This notebook is intended to evaluate the subset approaches for esnli, where the\n",
    "output is a dataframe that looks something like:\n",
    "\n",
    "| Classifier | Approach              | Coverage (% identified) | Validity (% identified and leads to flip) | Median Size |\n",
    "|------------|-----------------------|--------------------------|--------------------------------------------|-------------|\n",
    "| Random     | Class Exclusion       | x                        | x                                          | x           |\n",
    "| Logistic   | Fast                  |                          |                                            |             |\n",
    "| Logistic   | Slow                  |                          |                                            |             |\n",
    "| Logistic   | Fast + CE fallback    |                          |                                            |             |\n",
    "| Logistic   | Slow + CE fallback    |                          |                                            |             |\n",
    "| KNN        | Greedy                |                          |                                            |             |\n",
    "| KNN        | Greedy + CE fallback  |                          |                                            |             |\n",
    "| SVM        | Greedy                |                          |                                            |             |\n",
    "| SVM        | Greedy + CE fallback  |                          |                                            |             |\n",
    "| DT         | Greedy                |                          |                                            |             |\n",
    "| DT         | Greedy + CE fallback  |                          |                                            |             |\n",
    "| LMeans     | Greedy                |                          |                                            |             |\n",
    "| LMeans     | Greedy + CE fallback  |                          |                                            |             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# TODO: Think about plotting subset sizes against predicted probability? (confidence)\n",
    "\n",
    "DATASET_NAME = \"esnli\"\n",
    "LABEL_SPACE = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "MODEL_NAME = \"deberta-large\"\n",
    "SEED = 42\n",
    "POOLER = \"mean_with_attention\"\n",
    "LAYER = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator KNeighborsClassifier from version 1.1.1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LinearSVC from version 1.1.1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/samsoup/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Load Embeddings\n",
    "from data.embeddings import load_saved_embeddings\n",
    "import numpy as np\n",
    "train_embeddings = load_saved_embeddings(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    split=\"train\",\n",
    "    pooler=POOLER,\n",
    "    layer=LAYER\n",
    ")\n",
    "\n",
    "eval_embeddings = load_saved_embeddings(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    split=\"eval\",\n",
    "    pooler=POOLER,\n",
    "    layer=LAYER\n",
    ")\n",
    "\n",
    "test_embeddings = load_saved_embeddings(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    split=\"test\",\n",
    "    pooler=POOLER,\n",
    "    layer=LAYER\n",
    ")\n",
    "\n",
    "train_eval_embeddings = np.vstack([train_embeddings, eval_embeddings])\n",
    "\n",
    "\n",
    "## Load Classifiers\n",
    "from data.models import load_saved_wrapperbox_model\n",
    "knn_clf = load_saved_wrapperbox_model(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    pooler=POOLER,\n",
    "    wrapperbox=\"KNN\"\n",
    ")\n",
    "\n",
    "svm_clf = load_saved_wrapperbox_model(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    pooler=POOLER,\n",
    "    wrapperbox=\"SVM\",\n",
    ")\n",
    "\n",
    "dt_clf = load_saved_wrapperbox_model(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    pooler=POOLER,\n",
    "    wrapperbox=\"DecisionTree\",\n",
    ")\n",
    "\n",
    "lmeans_clf = load_saved_wrapperbox_model(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    pooler=POOLER,\n",
    "    wrapperbox=\"LMeans\",\n",
    ")\n",
    "\n",
    "## Load Datasets and Labels\n",
    "from data.datasets import load_dataset_from_hf, load_labels_at_split\n",
    "import numpy as np\n",
    "dataset = load_dataset_from_hf(dataset=DATASET_NAME)\n",
    "train_labels = load_labels_at_split(dataset, \"train\")\n",
    "eval_labels = load_labels_at_split(dataset, \"eval\")\n",
    "train_eval_labels = np.concatenate([train_labels, eval_labels])\n",
    "test_labels = load_labels_at_split(dataset, \"test\")\n",
    "\n",
    "from datasets import DatasetDict, concatenate_datasets\n",
    "train_eval_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"eval\"]])\n",
    "dataset_dict = DatasetDict(\n",
    "    {\"train\": train_eval_dataset, \"test\": dataset[\"test\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation functions\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from sklearn import clone\n",
    "from sklearn.base import BaseEstimator\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def retrain_and_evaluate_validity(\n",
    "    clf: BaseEstimator, \n",
    "    train_embeddings: np.ndarray, \n",
    "    train_labels: np.ndarray, \n",
    "    x_test: np.ndarray, \n",
    "    indices_to_exclude: np.ndarray\n",
    "):\n",
    "    train_mask = np.ones(train_embeddings.shape[0], dtype=bool)\n",
    "    train_mask[indices_to_exclude] = False\n",
    "    reduced_embeddings = train_embeddings[train_mask]\n",
    "    reduced_labels = train_labels[train_mask]\n",
    "    old_pred = clf.predict(x_test.reshape(1, -1))[0]\n",
    "    new_clf = clone(clf)\n",
    "    new_clf.fit(reduced_embeddings, reduced_labels)\n",
    "    new_pred = new_clf.predict(x_test.reshape(1, -1))[0]\n",
    "    # this subset is valid only if new prediction does not equal old prediction\n",
    "    return old_pred, new_pred, new_pred != old_pred\n",
    "\n",
    "def evaluate_predictions(\n",
    "    clf: BaseEstimator,\n",
    "    flip_list: List[List[int]],\n",
    "    train_embeddings: np.ndarray,\n",
    "    train_labels: np.ndarray, \n",
    "    test_embeddings: np.ndarray, \n",
    "    ex_indices_to_check: List[int], \n",
    "):\n",
    "    is_valid_subsets = []\n",
    "    for test_ex_idx in tqdm(ex_indices_to_check):\n",
    "        _, _, is_valid_subset = retrain_and_evaluate_validity(\n",
    "            clf=clf, \n",
    "            train_embeddings=train_embeddings, \n",
    "            train_labels=train_labels, \n",
    "            x_test=test_embeddings[test_ex_idx],\n",
    "            indices_to_exclude=flip_list[test_ex_idx]\n",
    "        )\n",
    "        is_valid_subsets.append(is_valid_subset)\n",
    "\n",
    "    return is_valid_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 0 proposed subsets, only 0.0 is valid\n",
      "Validity: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Do metrics for Yang fast\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "l2 = 500\n",
    "logit_clf = LogisticRegression(penalty=\"l2\", C= 1 / l2)\n",
    "logit_clf.fit(train_eval_embeddings, train_eval_labels)\n",
    "\n",
    "filename = \"esnli_deberta_large_yang2023_alg1.pickle\"\n",
    "with open(filename, 'rb') as handle:\n",
    "    yang_flip_list = pickle.load(handle)\n",
    "\n",
    "# filter flip list to num zero entry\n",
    "ex_indices = [i for i, l in enumerate(yang_flip_list) if l is not None]\n",
    "\n",
    "is_yang_valid = evaluate_predictions(\n",
    "    clf=logit_clf,\n",
    "    flip_list=yang_flip_list,\n",
    "    train_embeddings=train_eval_embeddings,\n",
    "    train_labels=train_eval_labels,\n",
    "    test_embeddings=test_embeddings,\n",
    "    ex_indices_to_check=ex_indices,\n",
    ")\n",
    "\n",
    "print(f\"Of the {len(ex_indices)} proposed subsets, only {np.sum(is_yang_valid)} is valid\")\n",
    "acc = np.sum(is_yang_valid)/len(test_labels) * 100\n",
    "print(f\"Validity: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_indices = [i for i, l in enumerate(yang_flip_list) if l is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = 'ex_indices_to_check_yang2023_alg1.pickle'\n",
    "with open(output_file_path, 'wb') as output_file:\n",
    "    pickle.dump(ex_indices, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8394"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ex_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yang_flip_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43myang_flip_list\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yang_flip_list' is not defined"
     ]
    }
   ],
   "source": [
    "yang_flip_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do metrics for Yang slow\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# l2 = 500\n",
    "# logit_clf = LogisticRegression(penalty=\"l2\", C= 1 / l2)\n",
    "# logit_clf.fit(train_eval_embeddings, train_eval_labels)\n",
    "\n",
    "filename = \"esnli_deberta_large_yang2023_alg2.pickle\"\n",
    "with open(filename, 'rb') as handle:\n",
    "    yang_flip_list = pickle.load(handle)\n",
    "\n",
    "ex_indices = []\n",
    "# filter flip list to num zero entry\n",
    "for i, l in enumerate(yang_flip_list):\n",
    "    if l is not None and len(l) > 0:\n",
    "        ex_indices.append(i)\n",
    "\n",
    "num_examples = []\n",
    "# compute some basic statistics\n",
    "for i, l in enumerate(yang_flip_list):\n",
    "    if l is None or len(l) == 0:\n",
    "        continue\n",
    "    # compute the length of indices\n",
    "    num_examples.append(len(l))\n",
    "\n",
    "# is_yang_valid = evaluate_predictions(\n",
    "#     clf=logit_clf,\n",
    "#     flip_list=yang_flip_list,\n",
    "#     train_embeddings=train_eval_embeddings,\n",
    "#     train_labels=train_eval_labels,\n",
    "#     test_embeddings=test_embeddings,\n",
    "#     ex_indices_to_check=ex_indices,\n",
    "# )\n",
    "\n",
    "# print(f\"Of the {len(ex_indices)} proposed subsets, only {np.sum(is_yang_valid)} is valid\")\n",
    "# acc = np.sum(is_yang_valid)/len(test_labels) * 100\n",
    "# print(f\"Validity: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7734, 7734, 9824)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num_examples), len(ex_indices), len(yang_flip_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9824"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ex_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = 'ex_indices_to_check_yang2023_alg2.pickle'\n",
    "with open(output_file_path, 'wb') as output_file:\n",
    "    pickle.dump(ex_indices, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7734"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ex_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8394"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file_path = 'ex_indices_to_check_yang2023_alg1.pickle'\n",
    "with open(output_file_path, 'rb') as output_file:\n",
    "    alg2_indices = pickle.load(output_file)\n",
    "\n",
    "len(alg2_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrapperbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
