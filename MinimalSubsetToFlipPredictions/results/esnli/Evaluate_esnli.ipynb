{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Approaches\n",
    "\n",
    "This notebook is intended to evaluate the subset approaches for esnli, where the\n",
    "output is a dataframe that looks something like:\n",
    "\n",
    "| Classifier | Approach              | Coverage (% identified) | Validity (% identified and leads to flip) | Median Size |\n",
    "|------------|-----------------------|--------------------------|--------------------------------------------|-------------|\n",
    "| Random     | Class Exclusion       | x                        | x                                          | x           |\n",
    "| Logistic   | Fast                  |                          |                                            |             |\n",
    "| Logistic   | Slow                  |                          |                                            |             |\n",
    "| Logistic   | Fast + CE fallback    |                          |                                            |             |\n",
    "| Logistic   | Slow + CE fallback    |                          |                                            |             |\n",
    "| KNN        | Greedy                |                          |                                            |             |\n",
    "| KNN        | Greedy + CE fallback  |                          |                                            |             |\n",
    "| SVM        | Greedy                |                          |                                            |             |\n",
    "| SVM        | Greedy + CE fallback  |                          |                                            |             |\n",
    "| DT         | Greedy                |                          |                                            |             |\n",
    "| DT         | Greedy + CE fallback  |                          |                                            |             |\n",
    "| LMeans     | Greedy                |                          |                                            |             |\n",
    "| LMeans     | Greedy + CE fallback  |                          |                                            |             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# TODO: Think about plotting subset sizes against predicted probability? (confidence)\n",
    "\n",
    "DATASET_NAME = \"esnli\"\n",
    "LABEL_SPACE = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "MODEL_NAME = \"deberta_large\"\n",
    "SEED = 42\n",
    "POOLER = \"mean_with_attention\"\n",
    "LAYER = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator KNeighborsClassifier from version 1.1.1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LinearSVC from version 1.1.1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/samsoup/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Load Embeddings\n",
    "from utils.io import (\n",
    "    load_dataset_from_hf,\n",
    "    load_labels_at_split,\n",
    "    load_embeddings,\n",
    "    load_wrapperbox\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_embeddings = load_embeddings(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    split=\"train\",\n",
    "    pooler=POOLER,\n",
    "    layer=LAYER\n",
    ")\n",
    "\n",
    "eval_embeddings = load_embeddings(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    split=\"eval\",\n",
    "    pooler=POOLER,\n",
    "    layer=LAYER\n",
    ")\n",
    "\n",
    "test_embeddings = load_embeddings(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    split=\"test\",\n",
    "    pooler=POOLER,\n",
    "    layer=LAYER\n",
    ")\n",
    "\n",
    "train_eval_embeddings = np.vstack([train_embeddings, eval_embeddings])\n",
    "\n",
    "\n",
    "## Load Classifiers\n",
    "knn_clf = load_wrapperbox(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    pooler=POOLER,\n",
    "    wrapperbox=\"KNN\"\n",
    ")\n",
    "\n",
    "svm_clf = load_wrapperbox(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    pooler=POOLER,\n",
    "    wrapperbox=\"SVM\",\n",
    ")\n",
    "\n",
    "dt_clf = load_wrapperbox(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    pooler=POOLER,\n",
    "    wrapperbox=\"DecisionTree\",\n",
    ")\n",
    "\n",
    "lmeans_clf = load_wrapperbox(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    pooler=POOLER,\n",
    "    wrapperbox=\"LMeans\",\n",
    ")\n",
    "\n",
    "## Load Datasets and Labels\n",
    "dataset = load_dataset_from_hf(dataset=DATASET_NAME)\n",
    "train_labels = load_labels_at_split(dataset, \"train\")\n",
    "eval_labels = load_labels_at_split(dataset, \"eval\")\n",
    "train_eval_labels = np.concatenate([train_labels, eval_labels])\n",
    "test_labels = load_labels_at_split(dataset, \"test\")\n",
    "\n",
    "from datasets import DatasetDict, concatenate_datasets\n",
    "train_eval_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"eval\"]])\n",
    "dataset_dict = DatasetDict(\n",
    "    {\"train\": train_eval_dataset, \"test\": dataset[\"test\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 0 proposed subsets, only 0.0 is valid\n",
      "Validity: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Do metrics for Yang fast\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from MinimalSubsetToFlipPredictions.evaluate import evaluate_predictions\n",
    "\n",
    "\n",
    "l2 = 500\n",
    "logit_clf = LogisticRegression(penalty=\"l2\", C= 1 / l2)\n",
    "logit_clf.fit(train_eval_embeddings, train_eval_labels)\n",
    "\n",
    "filename = \"esnli_deberta_large_yang2023_alg1.pickle\"\n",
    "with open(filename, 'rb') as handle:\n",
    "    yang_flip_list = pickle.load(handle)\n",
    "\n",
    "# filter flip list to num zero entry\n",
    "ex_indices = [i for i, l in enumerate(yang_flip_list) if l is not None]\n",
    "\n",
    "is_yang_valid = evaluate_predictions(\n",
    "    clf=logit_clf,\n",
    "    flip_list=yang_flip_list,\n",
    "    train_embeddings=train_eval_embeddings,\n",
    "    train_labels=train_eval_labels,\n",
    "    test_embeddings=test_embeddings,\n",
    "    ex_indices_to_check=ex_indices,\n",
    ")\n",
    "\n",
    "print(f\"Of the {len(ex_indices)} proposed subsets, only {np.sum(is_yang_valid)} is valid\")\n",
    "acc = np.sum(is_yang_valid)/len(test_labels) * 100\n",
    "print(f\"Validity: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do metrics for Yang slow\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from MinimalSubsetToFlipPredictions.evaluate import evaluate_predictions\n",
    "\n",
    "\n",
    "filename = \"esnli_deberta_large_yang2023_alg2.pickle\"\n",
    "with open(filename, 'rb') as handle:\n",
    "    yang_flip_list = pickle.load(handle)\n",
    "\n",
    "ex_indices = []\n",
    "# filter flip list to num zero entry\n",
    "for i, l in enumerate(yang_flip_list):\n",
    "    if l is not None and len(l) > 0:\n",
    "        ex_indices.append(i)\n",
    "\n",
    "num_examples = []\n",
    "# compute some basic statistics\n",
    "for i, l in enumerate(yang_flip_list):\n",
    "    if l is None or len(l) == 0:\n",
    "        continue\n",
    "    # compute the length of indices\n",
    "    num_examples.append(len(l))\n",
    "\n",
    "# is_yang_valid = evaluate_predictions(\n",
    "#     clf=logit_clf,\n",
    "#     flip_list=yang_flip_list,\n",
    "#     train_embeddings=train_eval_embeddings,\n",
    "#     train_labels=train_eval_labels,\n",
    "#     test_embeddings=test_embeddings,\n",
    "#     ex_indices_to_check=ex_indices,\n",
    "# )\n",
    "\n",
    "# print(f\"Of the {len(ex_indices)} proposed subsets, only {np.sum(is_yang_valid)} is valid\")\n",
    "# acc = np.sum(is_yang_valid)/len(test_labels) * 100\n",
    "# print(f\"Validity: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'knn_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m wrapper_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKNN\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m flip_list \u001b[38;5;241m=\u001b[39m load_pickle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwrapper_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m is_valid \u001b[38;5;241m=\u001b[39m evaluate_predictions(\n\u001b[0;32m---> 10\u001b[0m     clf\u001b[38;5;241m=\u001b[39m\u001b[43mknn_clf\u001b[49m,\n\u001b[1;32m     11\u001b[0m     flip_list\u001b[38;5;241m=\u001b[39mflip_list,\n\u001b[1;32m     12\u001b[0m     train_embeddings\u001b[38;5;241m=\u001b[39mtrain_eval_embeddings,\n\u001b[1;32m     13\u001b[0m     train_labels\u001b[38;5;241m=\u001b[39mtrain_eval_labels,\n\u001b[1;32m     14\u001b[0m     test_embeddings\u001b[38;5;241m=\u001b[39mtest_embeddings,\n\u001b[1;32m     15\u001b[0m     ex_indices_to_check\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(test_labels\u001b[38;5;241m.\u001b[39msize),\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'knn_clf' is not defined"
     ]
    }
   ],
   "source": [
    "## Do metrics for KNN\n",
    "\n",
    "from MinimalSubsetToFlipPredictions.evaluate import evaluate_predictions\n",
    "from utils.io import load_pickle\n",
    "\n",
    "wrapper_name = \"KNN\"\n",
    "flip_list = load_pickle(f\"{DATASET_NAME}_{MODEL_NAME}_{wrapper_name}.pickle\")\n",
    "\n",
    "is_valid = evaluate_predictions(\n",
    "    clf=knn_clf,\n",
    "    flip_list=flip_list,\n",
    "    train_embeddings=train_eval_embeddings,\n",
    "    train_labels=train_eval_labels,\n",
    "    test_embeddings=test_embeddings,\n",
    "    ex_indices_to_check=np.arange(test_labels.size),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do metrics for LGBM\n",
    "\n",
    "from MinimalSubsetToFlipPredictions.evaluate import evaluate_predictions\n",
    "from utils.io import load_pickle\n",
    "\n",
    "wrapper_name = \"LGBM\"\n",
    "flip_list = load_pickle(f\"{DATASET_NAME}_{MODEL_NAME}_{wrapper_name}.pickle\")\n",
    "\n",
    "is_valid = evaluate_predictions(\n",
    "    clf=knn_clf,\n",
    "    flip_list=flip_list,\n",
    "    train_embeddings=train_eval_embeddings,\n",
    "    train_labels=train_eval_labels,\n",
    "    test_embeddings=test_embeddings,\n",
    "    ex_indices_to_check=np.arange(test_labels.size),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "## Do metrics for LMeans\n",
    "\n",
    "from MinimalSubsetToFlipPredictions.evaluate import evaluate_predictions\n",
    "from utils.io import load_pickle\n",
    "\n",
    "wrapper_name = \"LMeans\"\n",
    "flip_list = load_pickle(f\"{DATASET_NAME}_{MODEL_NAME}_{wrapper_name}.pickle\")\n",
    "\n",
    "is_valid = evaluate_predictions(\n",
    "    clf=knn_clf,\n",
    "    flip_list=flip_list,\n",
    "    train_embeddings=train_eval_embeddings,\n",
    "    train_labels=train_eval_labels,\n",
    "    test_embeddings=test_embeddings,\n",
    "    ex_indices_to_check=np.arange(test_labels.size),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrapperbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
