{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled list saved to MinimalSubset/yang_fast/esnli_deberta_large_yang2023_alg1.pickle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Define the base directory and the pattern of folder names\n",
    "base_dir = 'MinimalSubset/yang_fast'\n",
    "incr = 350\n",
    "end = 10150\n",
    "folders = [f\"{i}to{i+incr}\" for i in range(0, end, incr)]\n",
    "\n",
    "# Initialize an empty list to hold all numpy arrays\n",
    "compiled_list = []\n",
    "\n",
    "# Iterate through each folder and load the pickle file\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    pickle_file_path = os.path.join(folder_path, 'yang2023_alg1_esnli500.pickle')\n",
    "    \n",
    "    if os.path.exists(pickle_file_path):\n",
    "        with open(pickle_file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            compiled_list.extend(data)\n",
    "    else:\n",
    "        print(f\"Pickle file not found: {pickle_file_path}\")\n",
    "\n",
    "# Save the compiled list to a new pickle file\n",
    "output_file_path = os.path.join(base_dir, 'esnli_deberta_large_yang2023_alg1.pickle')\n",
    "with open(output_file_path, 'wb') as output_file:\n",
    "    pickle.dump(compiled_list, output_file)\n",
    "\n",
    "print(f\"Compiled list saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled list saved to MinimalSubset/yang_slow/esnli_deberta_large_yang2023_alg2.pickle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Define the base directory and the pattern of folder names\n",
    "base_dir = 'MinimalSubset/yang_slow'\n",
    "incr = 450\n",
    "end = 9900\n",
    "filename = \"yang2023_alg2_esnli500_LR_I_D1\"\n",
    "folders = [f\"{i}to{i+incr}\" for i in range(0, end, incr)]\n",
    "\n",
    "# Initialize an empty list to hold all numpy arrays\n",
    "compiled_list = []\n",
    "\n",
    "# Iterate through each folder and load the pickle file\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    pickle_file_path = os.path.join(folder_path, f'{filename}.pickle')\n",
    "    \n",
    "    if os.path.exists(pickle_file_path):\n",
    "        with open(pickle_file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            compiled_list.extend(data)\n",
    "    else:\n",
    "        print(f\"Pickle file not found: {pickle_file_path}\")\n",
    "\n",
    "# Save the compiled list to a new pickle file\n",
    "output_file_path = os.path.join(base_dir, 'esnli_deberta_large_yang2023_alg2.pickle')\n",
    "with open(output_file_path, 'wb') as output_file:\n",
    "    pickle.dump(compiled_list, output_file)\n",
    "\n",
    "print(f\"Compiled list saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from utils.io import load_json\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define the base directory and the pattern of folder names\n",
    "base_dir = 'MinimalSubset'\n",
    "incr = 55\n",
    "end = 9824\n",
    "\n",
    "compiled_list = []\n",
    "for i in range(0, end, incr):\n",
    "    e = min(end, i + incr)\n",
    "    filename = f\"{i}to{e}_esnli_deberta-large_LMeans_minimal_subsets.json\"\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        js = load_json(file_path)\n",
    "        compiled_list.extend(js)\n",
    "    else:\n",
    "        print(f\"Json file not found: {file_path}\")\n",
    "        \n",
    "# Save the compiled list to a new json file\n",
    "output_file_path = os.path.join(base_dir, 'esnli_deberta_large_LMeans_minimal_subsets.json')\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(compiled_list, output_file)\n",
    "\n",
    "print(f\"Compiled list saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled list saved to LGBM_compiled/esnli_deberta_large_LGBM_minimal_subsets.json\n"
     ]
    }
   ],
   "source": [
    "from utils.io import load_json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Define the base directory and the pattern of folder names\n",
    "base_dir = 'LGBM_compiled'\n",
    "incr = 700\n",
    "end = 9824\n",
    "folders = [f\"{i}to{i+incr}\" for i in range(0, end, incr)]\n",
    "\n",
    "# Initialize an empty list to hold all numpy arrays\n",
    "compiled_list = []\n",
    "\n",
    "for i in range(0, end, incr):\n",
    "    e = min(end, i + incr)\n",
    "    filename = f\"{i}to{e}_esnli_deberta-large_LGBM_minimal_subsets.json\"\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        js = load_json(file_path)\n",
    "        compiled_list.extend(js)\n",
    "    else:\n",
    "        print(f\"Json file not found: {file_path}\")\n",
    "\n",
    "# Save the compiled list to a new pickle file\n",
    "output_file_path = os.path.join(base_dir, 'esnli_deberta_large_LGBM_minimal_subsets.json')\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(compiled_list, output_file)\n",
    "\n",
    "# with open(output_file_path, 'wb') as output_file:\n",
    "#     pickle.dump(compiled_list, output_file)\n",
    "\n",
    "print(f\"Compiled list saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Define the base directory and the pattern of folder names\n",
    "base_dir = 'MinimalSubset/yang_slow'\n",
    "incr = 450\n",
    "end = 9900\n",
    "filename = \"yang2023_alg2_esnli500_LR_I_D1\"\n",
    "folders = [f\"{i}to{i+incr}\" for i in range(0, end, incr)]\n",
    "\n",
    "# Initialize an empty list to hold all numpy arrays\n",
    "compiled_list = []\n",
    "\n",
    "# Iterate through each folder and load the pickle file\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    pickle_file_path = os.path.join(folder_path, f'{filename}.pickle')\n",
    "    \n",
    "    if os.path.exists(pickle_file_path):\n",
    "        with open(pickle_file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            compiled_list.extend(data)\n",
    "    else:\n",
    "        print(f\"Pickle file not found: {pickle_file_path}\")\n",
    "\n",
    "# Save the compiled list to a new pickle file\n",
    "output_file_path = os.path.join(base_dir, 'esnli_deberta_large_yang2023_alg2.pickle')\n",
    "with open(output_file_path, 'wb') as output_file:\n",
    "    pickle.dump(compiled_list, output_file)\n",
    "\n",
    "print(f\"Compiled list saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import math\n",
    "\n",
    "def split_pickle_file(input_filename, n):\n",
    "    # Load the large pickle file\n",
    "    with open(input_filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # Calculate the size of each chunk\n",
    "    total_size = len(data)\n",
    "    chunk_size = math.ceil(total_size / n)\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    output_dir = os.path.dirname(input_filename)\n",
    "    base_filename = os.path.splitext(os.path.basename(input_filename))[0]\n",
    "    \n",
    "    # Split and save the data into n parts\n",
    "    for i in range(n):\n",
    "        chunk = data[i*chunk_size:(i+1)*chunk_size]\n",
    "        output_filename = os.path.join(output_dir, f\"{base_filename}_part_{i+1}.pickle\")\n",
    "        with open(output_filename, 'wb') as f:\n",
    "            pickle.dump(chunk, f)\n",
    "        print(f\"Saved chunk {i+1} to {output_filename}\")\n",
    "\n",
    "# Example usage\n",
    "input_filename = 'esnli_deberta_large_LMeans.pickle'\n",
    "n = 5  # Number of parts to split into\n",
    "split_pickle_file(input_filename, n)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrapperbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
