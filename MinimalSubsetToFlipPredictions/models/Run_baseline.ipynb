{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Runner\n",
    "\n",
    "Runs the baseline for each of the proposed method:\n",
    "\n",
    "| Classifier | Approach              | Coverage (% identified) | Validity (% identified and leads to flip) | Median Size |\n",
    "|------------|-----------------------|--------------------------|--------------------------------------------|-------------|\n",
    "| Logistic   | Class Exclusion       |                          |                                            |             |\n",
    "| Logistic   | Fast + CE fallback    |                          |                                            |             |\n",
    "| Logistic   | Slow + CE fallback    |                          |                                            |             |\n",
    "| KNN        | Class Exclusion       |                          |                                            |             |\n",
    "| KNN        | Greedy + CE fallback  |                          |                                            |             |\n",
    "| SVM        | Class Exclusion       |                          |                                            |             |\n",
    "| SVM        | Greedy + CE fallback  |                          |                                            |             |\n",
    "| DT         | Class Exclusion       |                          |                                            |             |\n",
    "| DT         | Greedy + CE fallback  |                          |                                            |             |\n",
    "| LMeans     | Class Exclusion       |                          |                                            |             |\n",
    "| LMeans     | Greedy + CE fallback  |                          |                                            |             |\n",
    "\n",
    "Note that running class exclusion + a random baseline does not make any sense, \n",
    "since if the random classifier is deterministic (fixed seed), then any removal\n",
    "would not be valid. Otherwise, validity is stochastic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "DATASET_NAME = \"esnli\"\n",
    "LABEL_SPACE = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "MODEL_NAME = \"deberta_large\"\n",
    "SEED = 42\n",
    "POOLER = \"mean_with_attention\"\n",
    "LAYER = 24\n",
    "\n",
    "from MinimalSubsetToFlipPredictions.models.SimpleBaseline import FindMinimalSubsetSimpleBaseline\n",
    "\n",
    "\n",
    "ce_finder = FindMinimalSubsetSimpleBaseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/samsoup/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Load Embeddings\n",
    "from utils.io import (\n",
    "    load_dataset_from_hf,\n",
    "    load_labels_at_split,\n",
    "    load_embeddings,\n",
    ")\n",
    "import numpy as np\n",
    "train_embeddings = load_embeddings(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    split=\"train\",\n",
    "    pooler=POOLER,\n",
    "    layer=LAYER\n",
    ")\n",
    "\n",
    "eval_embeddings = load_embeddings(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    split=\"eval\",\n",
    "    pooler=POOLER,\n",
    "    layer=LAYER\n",
    ")\n",
    "\n",
    "test_embeddings = load_embeddings(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    split=\"test\",\n",
    "    pooler=POOLER,\n",
    "    layer=LAYER\n",
    ")\n",
    "\n",
    "train_eval_embeddings = np.vstack([train_embeddings, eval_embeddings])\n",
    "\n",
    "## Load Datasets and Labels\n",
    "dataset = load_dataset_from_hf(dataset=DATASET_NAME)\n",
    "train_labels = load_labels_at_split(dataset, \"train\")\n",
    "eval_labels = load_labels_at_split(dataset, \"eval\")\n",
    "train_eval_labels = np.concatenate([train_labels, eval_labels])\n",
    "test_labels = load_labels_at_split(dataset, \"test\")\n",
    "\n",
    "from datasets import DatasetDict, concatenate_datasets\n",
    "train_eval_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"eval\"]])\n",
    "dataset_dict = DatasetDict(\n",
    "    {\"train\": train_eval_dataset, \"test\": dataset[\"test\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Logistic with Class Exclusion\n",
    "from classifiers import RandomClassifier\n",
    "from utils.io import load_wrapperbox\n",
    "import pickle\n",
    "\n",
    "wrapper_name = \"LogisticRegression\"\n",
    "clf = load_wrapperbox(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    pooler=POOLER,\n",
    "    wrapperbox=wrapper_name\n",
    ")\n",
    "\n",
    "subsets = ce_finder.find_minimal_subset(\n",
    "    clf=clf, \n",
    "    train_embeddings=train_eval_embeddings, \n",
    "    test_embeddings=test_embeddings, \n",
    "    train_labels=train_eval_labels\n",
    ")\n",
    "\n",
    "output_file = f\"{DATASET_NAME}_{MODEL_NAME}_{wrapper_name}_baseline.pickle\"\n",
    "with open(output_file, 'wb') as handle:\n",
    "    pickle.dump(subsets, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Logistic with Yang fast + Class Exclusion as a fall back mechanism\n",
    "\n",
    "from utils.io import load_pickle\n",
    "\n",
    "\n",
    "name = \"yang2023_fast\"\n",
    "flip_list_filename = f\"{DATASET_NAME}_{MODEL_NAME}_{name}.pickle\"\n",
    "flip_list = load_pickle(flip_list_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator KNeighborsClassifier from version 1.1.1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Run KNN with Class Exclusion\n",
    "from classifiers import RandomClassifier\n",
    "from utils.io import load_wrapperbox\n",
    "import pickle\n",
    "\n",
    "wrapper_name = \"KNN\"\n",
    "clf = load_wrapperbox(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    pooler=POOLER,\n",
    "    wrapperbox=wrapper_name\n",
    ")\n",
    "\n",
    "subsets = ce_finder.find_minimal_subset(\n",
    "    clf=clf, \n",
    "    train_embeddings=train_eval_embeddings, \n",
    "    test_embeddings=test_embeddings, \n",
    "    train_labels=train_eval_labels\n",
    ")\n",
    "\n",
    "output_file = f\"{DATASET_NAME}_{MODEL_NAME}_{wrapper_name}_baseline.pickle\"\n",
    "with open(output_file, 'wb') as handle:\n",
    "    pickle.dump(subsets, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsoup/anaconda3/envs/wrapperbox/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LinearSVC from version 1.1.1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Run SVM with Class Exclusion\n",
    "from classifiers import RandomClassifier\n",
    "from utils.io import load_wrapperbox\n",
    "import pickle\n",
    "\n",
    "wrapper_name = \"SVM\"\n",
    "clf = load_wrapperbox(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    pooler=POOLER,\n",
    "    wrapperbox=wrapper_name\n",
    ")\n",
    "\n",
    "subsets = ce_finder.find_minimal_subset(\n",
    "    clf=clf, \n",
    "    train_embeddings=train_eval_embeddings, \n",
    "    test_embeddings=test_embeddings, \n",
    "    train_labels=train_eval_labels\n",
    ")\n",
    "\n",
    "output_file = f\"{DATASET_NAME}_{MODEL_NAME}_{wrapper_name}_baseline.pickle\"\n",
    "with open(output_file, 'wb') as handle:\n",
    "    pickle.dump(subsets, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run LGBM with Class Exclusion\n",
    "from classifiers import RandomClassifier\n",
    "from utils.io import load_wrapperbox\n",
    "import pickle\n",
    "\n",
    "wrapper_name = \"LGBM\"\n",
    "clf = load_wrapperbox(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    pooler=POOLER,\n",
    "    wrapperbox=wrapper_name\n",
    ")\n",
    "\n",
    "subsets = ce_finder.find_minimal_subset(\n",
    "    clf=clf, \n",
    "    train_embeddings=train_eval_embeddings, \n",
    "    test_embeddings=test_embeddings, \n",
    "    train_labels=train_eval_labels\n",
    ")\n",
    "\n",
    "output_file = f\"{DATASET_NAME}_{MODEL_NAME}_{wrapper_name}_baseline.pickle\"\n",
    "\n",
    "with open(output_file, 'wb') as handle:\n",
    "    pickle.dump(subsets, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run LMeans with Class Exclusion\n",
    "from classifiers import RandomClassifier\n",
    "from utils.io import load_wrapperbox\n",
    "import pickle\n",
    "\n",
    "wrapper_name = \"LMeans\"\n",
    "clf = load_wrapperbox(\n",
    "    dataset=DATASET_NAME,\n",
    "    model=MODEL_NAME,\n",
    "    seed=SEED,\n",
    "    pooler=POOLER,\n",
    "    wrapperbox=wrapper_name\n",
    ")\n",
    "\n",
    "subsets = ce_finder.find_minimal_subset(\n",
    "    clf=clf, \n",
    "    train_embeddings=train_eval_embeddings, \n",
    "    test_embeddings=test_embeddings, \n",
    "    train_labels=train_eval_labels\n",
    ")\n",
    "\n",
    "output_file = f\"{DATASET_NAME}_{MODEL_NAME}_{wrapper_name}_baseline.pickle\"\n",
    "with open(output_file, 'wb') as handle:\n",
    "    pickle.dump(subsets, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrapperbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
